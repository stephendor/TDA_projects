Validation Console Output - real_data_deep_tda_breakthrough
Timestamp: 20250806_143040
Random Seed: 42
================================================================================

[2025-08-06T14:32:09.790437]
STDOUT:
üöÄ REAL DATA DEEP TDA BREAKTHROUGH VALIDATION
================================================================================
Revolutionary approach: Deep TDA on real CIC-IDS2017 APT attacks
Target: 90%+ F1-score on actual infiltration attack patterns
Method: TDA-native deep learning preserving topological attack signatures
Dataset: Real APT infiltration attacks from CIC-IDS2017
================================================================================

üìä REAL APT DATA LOADING
üîß LOADING REAL CIC-IDS2017 INFILTRATION DATA
------------------------------------------------------------
Loading: /home/stephen-dorman/dev/TDA_projects/data/apt_datasets/cicids2017/MachineLearningCSV/MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv
‚úÖ Dataset loaded: (288602, 79)
Columns: 79

Label Distribution:
  BENIGN: 288566 (99.99%)
  Infiltration: 36 (0.01%)

Data cleaning:
  Initial NaN values: 18
  Initial Inf values: 396
  Final NaN values: 0

Label Encoding:
  0: BENIGN
  1: Infiltration

Selected 20/20 key features for TDA analysis

Final data shape: (288602, 20)
Label shape: (288602,)
Attack rate: 0.0001 (36 attacks)

üîÑ CREATING TEMPORAL SEQUENCES FROM REAL DATA
--------------------------------------------------
Input shape: (288602, 20)
Sequence length: 50
Overlap: 0.3
  Processed 1000 sequences...
  Processed 2000 sequences...
  Processed 3000 sequences...
  Processed 4000 sequences...
  Processed 5000 sequences...
  Processed 6000 sequences...
  Processed 7000 sequences...
  Processed 8000 sequences...
‚úÖ Created 8245 temporal sequences
   Sequence shape: (8245, 50, 20)
   Attack rate: 0.0061 (50 attack sequences)
   Benign sequences: 8195

‚öñÔ∏è DATASET BALANCING FOR TRAINING
Attack sequences available: 50
Benign sequences available: 8195
‚ö†Ô∏è WARNING: Very few attack sequences. Results may not be representative.
‚úÖ Balanced dataset:
   Total sequences: 200
   Attack sequences: 50 (25.0%)
   Benign sequences: 150 (75.0%)

üîß FEATURE SCALING
‚úÖ Features scaled: (200, 50, 20)
   Mean: -0.000000
   Std: 1.000000

üìä TRAIN-TEST SPLIT
Train: 160 sequences
  - Attack: 40 (25.0%)
  - Benign: 120 (75.0%)
Test: 40 sequences
  - Attack: 10 (25.0%)
  - Benign: 30 (75.0%)
‚úÖ Data loaders created with batch size: 32

üß† DEEP TDA MODEL INITIALIZATION
Model parameters: 5,264,903
Architecture: Differentiable PH + Persistent Attention + Transformer
Ready for real APT attack pattern learning

üöÄ TRAINING ON REAL APT DATA
Training on: cpu
  Batch 0/5: Loss = 0.7502
Epoch 1/50:
  Train Loss: 0.7316, Train Acc: 60.62%
  Val Loss: 0.6977, Val F1: 0.3479
  ‚úÖ New best model saved: F1 = 0.3479
------------------------------------------------------------
  Batch 0/5: Loss = 0.6828
Epoch 2/50:
  Train Loss: 0.6975, Train Acc: 31.88%
  Val Loss: 0.7055, Val F1: 0.4322
  ‚úÖ New best model saved: F1 = 0.4322
------------------------------------------------------------
  Batch 0/5: Loss = 0.6939
Epoch 3/50:
  Train Loss: 0.6882, Train Acc: 56.25%
  Val Loss: 0.7078, Val F1: 0.6691
  ‚úÖ New best model saved: F1 = 0.6691
------------------------------------------------------------
  Batch 0/5: Loss = 0.6745
Epoch 4/50:
  Train Loss: 0.6694, Train Acc: 71.88%
  Val Loss: 0.7102, Val F1: 0.6691
------------------------------------------------------------
  Batch 0/5: Loss = 0.6507
Epoch 5/50:
  Train Loss: 0.6745, Train Acc: 58.75%
  Val Loss: 0.7131, Val F1: 0.5092
------------------------------------------------------------
  Batch 0/5: Loss = 0.6504
Epoch 6/50:
  Train Loss: 0.6658, Train Acc: 56.25%
  Val Loss: 0.7113, Val F1: 0.5982
------------------------------------------------------------
  Batch 0/5: Loss = 0.6516
Epoch 7/50:
  Train Loss: 0.6539, Train Acc: 67.50%
  Val Loss: 0.7126, Val F1: 0.6500
------------------------------------------------------------
  Batch 0/5: Loss = 0.7121
Epoch 8/50:
  Train Loss: 0.6713, Train Acc: 63.12%
  Val Loss: 0.7117, Val F1: 0.6500
------------------------------------------------------------
  Batch 0/5: Loss = 0.6804
Epoch 9/50:
  Train Loss: 0.6621, Train Acc: 64.38%
  Val Loss: 0.7110, Val F1: 0.6500
------------------------------------------------------------
  Batch 0/5: Loss = 0.6700
Epoch 10/50:
  Train Loss: 0.6526, Train Acc: 65.62%
  Val Loss: 0.7111, Val F1: 0.6800
  ‚úÖ New best model saved: F1 = 0.6800
------------------------------------------------------------
  Batch 0/5: Loss = 0.6171
Epoch 11/50:
  Train Loss: 0.6460, Train Acc: 60.62%
  Val Loss: 0.7072, Val F1: 0.4048
------------------------------------------------------------
  Batch 0/5: Loss = 0.7146
Epoch 12/50:
  Train Loss: 0.6457, Train Acc: 59.38%
  Val Loss: 0.7117, Val F1: 0.5333
------------------------------------------------------------
  Batch 0/5: Loss = 0.6972
Epoch 13/50:
  Train Loss: 0.6293, Train Acc: 70.62%
  Val Loss: 0.7067, Val F1: 0.6691
------------------------------------------------------------
  Batch 0/5: Loss = 0.5851
Epoch 14/50:
  Train Loss: 0.6210, Train Acc: 70.62%
  Val Loss: 0.7091, Val F1: 0.6020
------------------------------------------------------------
  Batch 0/5: Loss = 0.6008
Epoch 15/50:
  Train Loss: 0.6014, Train Acc: 67.50%
  Val Loss: 0.7120, Val F1: 0.5797
------------------------------------------------------------
  Batch 0/5: Loss = 0.6185
Epoch 16/50:
  Train Loss: 0.6074, Train Acc: 67.50%
  Val Loss: 0.7204, Val F1: 0.6239
------------------------------------------------------------
  Batch 0/5: Loss = 0.5825
Epoch 17/50:
  Train Loss: 0.5871, Train Acc: 66.88%
  Val Loss: 0.7373, Val F1: 0.6667
------------------------------------------------------------
  Batch 0/5: Loss = 0.6365
Epoch 18/50:
  Train Loss: 0.5819, Train Acc: 70.62%
  Val Loss: 0.7237, Val F1: 0.6239
------------------------------------------------------------
  Batch 0/5: Loss = 0.4868
Epoch 19/50:
  Train Loss: 0.5723, Train Acc: 71.25%
  Val Loss: 0.7132, Val F1: 0.5797
------------------------------------------------------------
  Batch 0/5: Loss = 0.6240
Epoch 20/50:
  Train Loss: 0.5858, Train Acc: 70.00%
  Val Loss: 0.7111, Val F1: 0.6239
------------------------------------------------------------
  Batch 0/5: Loss = 0.6523
Epoch 21/50:
  Train Loss: 0.5807, Train Acc: 74.38%
  Val Loss: 0.6746, Val F1: 0.7085
  ‚úÖ New best model saved: F1 = 0.7085
------------------------------------------------------------
  Batch 0/5: Loss = 0.6051
Epoch 22/50:
  Train Loss: 0.5556, Train Acc: 74.38%
  Val Loss: 0.6824, Val F1: 0.7085
------------------------------------------------------------
  Batch 0/5: Loss = 0.4745
Epoch 23/50:
  Train Loss: 0.5325, Train Acc: 78.75%
  Val Loss: 0.6858, Val F1: 0.7179
  ‚úÖ New best model saved: F1 = 0.7179
------------------------------------------------------------
  Batch 0/5: Loss = 0.5603
Epoch 24/50:
  Train Loss: 0.5453, Train Acc: 64.38%
  Val Loss: 0.7167, Val F1: 0.6709
------------------------------------------------------------
  Batch 0/5: Loss = 0.4849
Epoch 25/50:
  Train Loss: 0.5176, Train Acc: 79.38%
  Val Loss: 0.8130, Val F1: 0.7709
  ‚úÖ New best model saved: F1 = 0.7709
------------------------------------------------------------
  Batch 0/5: Loss = 0.5752
Epoch 26/50:
  Train Loss: 0.4930, Train Acc: 85.00%
  Val Loss: 0.7507, Val F1: 0.7200
------------------------------------------------------------
  Batch 0/5: Loss = 0.4728
Epoch 27/50:
  Train Loss: 0.4852, Train Acc: 78.75%
  Val Loss: 0.7058, Val F1: 0.7292
------------------------------------------------------------
  Batch 0/5: Loss = 0.4904
Epoch 28/50:
  Train Loss: 0.4581, Train Acc: 80.62%
  Val Loss: 0.7469, Val F1: 0.7200
------------------------------------------------------------
  Batch 0/5: Loss = 0.4808
Epoch 29/50:
  Train Loss: 0.4621, Train Acc: 83.75%
  Val Loss: 0.7665, Val F1: 0.7200
------------------------------------------------------------
  Batch 0/5: Loss = 0.3811
Epoch 30/50:
  Train Loss: 0.4799, Train Acc: 82.50%
  Val Loss: 0.7713, Val F1: 0.7200
------------------------------------------------------------
  Batch 0/5: Loss = 0.3636
Epoch 31/50:
  Train Loss: 0.4652, Train Acc: 81.25%
  Val Loss: 0.8019, Val F1: 0.7143
------------------------------------------------------------
  Batch 0/5: Loss = 0.3330
Epoch 32/50:
  Train Loss: 0.4347, Train Acc: 81.25%
  Val Loss: 0.7281, Val F1: 0.7357
------------------------------------------------------------
  Batch 0/5: Loss = 0.3362
Epoch 33/50:
  Train Loss: 0.4034, Train Acc: 81.25%
  Val Loss: 0.6729, Val F1: 0.7838
  ‚úÖ New best model saved: F1 = 0.7838
------------------------------------------------------------
  Batch 0/5: Loss = 0.5898
Epoch 34/50:
  Train Loss: 0.4356, Train Acc: 83.12%
  Val Loss: 0.7544, Val F1: 0.7401
------------------------------------------------------------
  Batch 0/5: Loss = 0.4003
Epoch 35/50:
  Train Loss: 0.3731, Train Acc: 83.75%
  Val Loss: 0.8164, Val F1: 0.7401
------------------------------------------------------------
  Batch 0/5: Loss = 0.6052
Epoch 36/50:
  Train Loss: 0.3805, Train Acc: 83.75%
  Val Loss: 0.7716, Val F1: 0.7785
------------------------------------------------------------
  Batch 0/5: Loss = 0.3468
Epoch 37/50:
  Train Loss: 0.3675, Train Acc: 85.62%
  Val Loss: 0.8162, Val F1: 0.7401
------------------------------------------------------------
  Batch 0/5: Loss = 0.2849
Epoch 38/50:
  Train Loss: 0.3559, Train Acc: 86.25%
  Val Loss: 0.7356, Val F1: 0.8000
  ‚úÖ New best model saved: F1 = 0.8000
------------------------------------------------------------
  Batch 0/5: Loss = 0.4025
Epoch 39/50:
  Train Loss: 0.3455, Train Acc: 88.12%
  Val Loss: 0.7168, Val F1: 0.8000
------------------------------------------------------------
  Batch 0/5: Loss = 0.2889
Epoch 40/50:
  Train Loss: 0.3515, Train Acc: 86.25%
  Val Loss: 0.7341, Val F1: 0.8000
------------------------------------------------------------
  Batch 0/5: Loss = 0.2538
Epoch 41/50:
  Train Loss: 0.3631, Train Acc: 90.00%
  Val Loss: 0.7376, Val F1: 0.7709
------------------------------------------------------------
  Batch 0/5: Loss = 0.2172
Epoch 42/50:
  Train Loss: 0.3195, Train Acc: 88.75%
  Val Loss: 0.7289, Val F1: 0.8000
------------------------------------------------------------
  Batch 0/5: Loss = 0.2968
Epoch 43/50:
  Train Loss: 0.3029, Train Acc: 90.00%
  Val Loss: 0.7031, Val F1: 0.7571
------------------------------------------------------------
  Batch 0/5: Loss = 0.3130
Epoch 44/50:
  Train Loss: 0.3113, Train Acc: 92.50%
  Val Loss: 0.8537, Val F1: 0.7921
------------------------------------------------------------
  Batch 0/5: Loss = 0.3039
Epoch 45/50:
  Train Loss: 0.2742, Train Acc: 90.62%
  Val Loss: 0.7111, Val F1: 0.7400
------------------------------------------------------------
  Batch 0/5: Loss = 0.3188
Epoch 46/50:
  Train Loss: 0.2560, Train Acc: 89.38%
  Val Loss: 0.7219, Val F1: 0.7921
------------------------------------------------------------
  Batch 0/5: Loss = 0.2065
Epoch 47/50:
  Train Loss: 0.2513, Train Acc: 92.50%
  Val Loss: 0.6178, Val F1: 0.8056
  ‚úÖ New best model saved: F1 = 0.8056
------------------------------------------------------------
  Batch 0/5: Loss = 0.1517
Epoch 48/50:
  Train Loss: 0.2825, Train Acc: 89.38%
  Val Loss: 0.6608, Val F1: 0.8056
------------------------------------------------------------
  Batch 0/5: Loss = 0.1981
Epoch 49/50:
  Train Loss: 0.2485, Train Acc: 92.50%
  Val Loss: 0.7290, Val F1: 0.7921
------------------------------------------------------------
  Batch 0/5: Loss = 0.2216
Epoch 50/50:
  Train Loss: 0.2466, Train Acc: 93.75%
  Val Loss: 0.7310, Val F1: 0.7921
------------------------------------------------------------

üéØ FINAL EVALUATION ON REAL APT DATA
============================================================
üèÜ BREAKTHROUGH RESULTS ON REAL APT DATA
=================================================================
Final F1-Score: 0.8056 (80.6%)
üìä STRONG PROGRESS: Significant improvement on real data!

Detailed Performance Analysis:
              precision    recall  f1-score   support

      Benign      0.893     0.833     0.862        30
      Attack      0.583     0.700     0.636        10

    accuracy                          0.800        40
   macro avg      0.738     0.767     0.749        40
weighted avg      0.815     0.800     0.806        40


üéØ APT Attack Detection Performance:
   Attack Precision: 0.583 (58.3%)
   Attack Recall: 0.700 (70.0%)
   Attack F1-Score: 0.636

----------------------------------------
