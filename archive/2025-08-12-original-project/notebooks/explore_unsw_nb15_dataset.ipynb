{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNSW-NB15 Dataset Exploration for TDA\n",
    "\n",
    "**Purpose**: Analyze UNSW-NB15 dataset structure and suitability for topological data analysis methods.\n",
    "\n",
    "**Key Questions**:\n",
    "1. What are the feature types and distributions?\n",
    "2. How are attack types distributed?\n",
    "3. What temporal structure exists?\n",
    "4. Which features are suitable for TDA methods?\n",
    "5. How does this compare to CTDAPD dataset limitations?\n",
    "\n",
    "**Expected Outcome**: Clear dataset assessment and TDA method recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"üìä UNSW-NB15 Dataset Exploration for TDA\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load UNSW-NB15 dataset\n",
    "data_path = Path(\"../data/apt_datasets/UNSW-NB15\")\n",
    "print(f\"üìÅ Looking for data in: {data_path}\")\n",
    "\n",
    "# Check available files\n",
    "if data_path.exists():\n",
    "    files = list(data_path.glob(\"*.parquet\")) + list(data_path.glob(\"*.csv\"))\n",
    "    print(f\"üìÑ Found files: {[f.name for f in files]}\")\n",
    "else:\n",
    "    print(\"‚ùå Data directory not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing sets\n",
    "try:\n",
    "    train_df = pd.read_parquet(data_path / \"UNSW_NB15_training-set.parquet\")\n",
    "    test_df = pd.read_parquet(data_path / \"UNSW_NB15_testing-set.parquet\")\n",
    "    \n",
    "    print(f\"‚úÖ Training set: {train_df.shape}\")\n",
    "    print(f\"‚úÖ Testing set: {test_df.shape}\")\n",
    "    print(f\"üìä Total samples: {len(train_df) + len(test_df)}\")\n",
    "    \n",
    "    # Combine for full analysis\n",
    "    df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "    print(f\"üîó Combined dataset: {df.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset overview\n",
    "if df is not None:\n",
    "    print(\"üìã DATASET OVERVIEW\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {len(df.columns)}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    print(\"\\nüìù Column Names:\")\n",
    "    for i, col in enumerate(df.columns):\n",
    "        print(f\"{i+1:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack type analysis\n",
    "if df is not None and 'attack_cat' in df.columns:\n",
    "    print(\"üéØ ATTACK TYPE DISTRIBUTION\")\n",
    "    attack_counts = df['attack_cat'].value_counts()\n",
    "    print(attack_counts)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    attack_counts.plot(kind='bar', rot=45)\n",
    "    plt.title('UNSW-NB15 Attack Type Distribution')\n",
    "    plt.xlabel('Attack Category')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compare with CTDAPD limitations\n",
    "    print(f\"\\n‚úÖ Attack diversity: {len(attack_counts)} categories\")\n",
    "    print(f\"‚úÖ Balanced distribution: Min/Max ratio = {attack_counts.min()/attack_counts.max():.3f}\")\n",
    "    \n",
    "elif df is not None and 'Label' in df.columns:\n",
    "    print(\"üéØ LABEL DISTRIBUTION\")\n",
    "    label_counts = df['Label'].value_counts()\n",
    "    print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature analysis for TDA suitability\n",
    "if df is not None:\n",
    "    print(\"üî¨ FEATURE ANALYSIS FOR TDA SUITABILITY\")\n",
    "    \n",
    "    # Identify numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    print(f\"üìä Numeric features: {len(numeric_cols)}\")\n",
    "    \n",
    "    # Remove target columns\n",
    "    target_cols = ['label', 'Label', 'attack_cat', 'id']\n",
    "    feature_cols = [col for col in numeric_cols if col.lower() not in [t.lower() for t in target_cols]]\n",
    "    print(f\"üéØ Feature columns for TDA: {len(feature_cols)}\")\n",
    "    \n",
    "    # Feature statistics\n",
    "    print(\"\\nüìà Feature Statistics Summary:\")\n",
    "    feature_stats = df[feature_cols].describe()\n",
    "    print(feature_stats.iloc[[0, 1, 3, 7]].T)  # count, mean, std, max\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_vals = df[feature_cols].isnull().sum()\n",
    "    if missing_vals.sum() > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è Missing values found in {missing_vals[missing_vals > 0].shape[0]} columns\")\n",
    "        print(missing_vals[missing_vals > 0])\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No missing values in numeric features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal analysis (if timestamp columns exist)\n",
    "if df is not None:\n",
    "    print(\"‚è∞ TEMPORAL STRUCTURE ANALYSIS\")\n",
    "    \n",
    "    # Look for timestamp/time columns\n",
    "    time_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                for keyword in ['time', 'date', 'timestamp', 'start', 'dur'])]\n",
    "    \n",
    "    if time_cols:\n",
    "        print(f\"üìÖ Time-related columns: {time_cols}\")\n",
    "        \n",
    "        for col in time_cols:\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  Type: {df[col].dtype}\")\n",
    "            print(f\"  Range: {df[col].min()} to {df[col].max()}\")\n",
    "            print(f\"  Unique values: {df[col].nunique()}\")\n",
    "    else:\n",
    "        print(\"‚ùå No clear timestamp columns found\")\n",
    "        print(\"‚ö†Ô∏è This may limit temporal network analysis methods\")\n",
    "        \n",
    "    # Network flow features (good for TDA)\n",
    "    flow_keywords = ['dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss', 'dloss', \n",
    "                    'sload', 'dload', 'spkts', 'dpkts', 'swin', 'dwin']\n",
    "    flow_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in flow_keywords)]\n",
    "    \n",
    "    print(f\"\\nüåä Network flow features: {len(flow_cols)}\")\n",
    "    print(f\"‚úÖ These are excellent for TDA methods: {flow_cols[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TDA Method Recommendations\n",
    "if df is not None:\n",
    "    print(\"üéØ TDA METHOD RECOMMENDATIONS FOR UNSW-NB15\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    n_samples = len(df)\n",
    "    n_features = len(feature_cols) if 'feature_cols' in locals() else 0\n",
    "    n_attacks = len(df[df.columns[df.columns.str.contains('attack|label', case=False, regex=True)]].iloc[:, 0].unique()) if any(df.columns.str.contains('attack|label', case=False, regex=True)) else 0\n",
    "    \n",
    "    print(f\"üìä Dataset Scale: {n_samples:,} samples, {n_features} features, {n_attacks} classes\")\n",
    "    \n",
    "    # Method recommendations based on dataset characteristics\n",
    "    recommendations = []\n",
    "    \n",
    "    if n_samples > 100000:\n",
    "        recommendations.append(\"‚úÖ PERSISTENT HOMOLOGY: Large sample size suitable for robust topology\")\n",
    "        recommendations.append(\"‚úÖ MAPPER: Can handle large-scale data with proper filtering\")\n",
    "    \n",
    "    if n_features >= 20:\n",
    "        recommendations.append(\"‚úÖ VIETORIS-RIPS COMPLEX: Rich feature space for simplicial analysis\")\n",
    "        recommendations.append(\"‚úÖ DIMENSION REDUCTION + TDA: Use PCA/UMAP preprocessing\")\n",
    "    \n",
    "    if time_cols:\n",
    "        recommendations.append(\"‚úÖ SLIDING WINDOW TDA: Temporal structure available\")\n",
    "        recommendations.append(\"‚úÖ COLLINS NETWORK METHOD: Good temporal connectivity\")\n",
    "    else:\n",
    "        recommendations.append(\"‚ö†Ô∏è STATIC TOPOLOGY ONLY: No temporal analysis methods\")\n",
    "    \n",
    "    if n_attacks > 5:\n",
    "        recommendations.append(\"‚úÖ MULTI-CLASS TDA: Rich attack diversity for topological signatures\")\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        print(rec)\n",
    "    \n",
    "    print(f\"\\nüèÜ RECOMMENDED PRIORITY ORDER:\")\n",
    "    print(f\"1. Enhanced Topological Dissimilarity (proven 5% baseline)\")\n",
    "    print(f\"2. Persistent Homology on Network Flow Features\")\n",
    "    print(f\"3. Mapper Analysis for Attack Pattern Visualization\")\n",
    "    if time_cols:\n",
    "        print(f\"4. Collins Network Structure Method (if temporal density sufficient)\")\n",
    "    \n",
    "    print(f\"\\nüìà EXPECTED IMPROVEMENTS vs CTDAPD:\")\n",
    "    print(f\"‚Ä¢ Richer feature space: {n_features} vs 15 features\")\n",
    "    print(f\"‚Ä¢ Better attack diversity: {n_attacks} vs 5 categories\")\n",
    "    print(f\"‚Ä¢ Larger sample size: {n_samples:,} vs 54,768\")\n",
    "    print(f\"‚Ä¢ Established benchmark: Literature comparisons available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation analysis (important for TDA)\n",
    "if df is not None and 'feature_cols' in locals() and len(feature_cols) > 0:\n",
    "    print(\"üîó FEATURE CORRELATION ANALYSIS\")\n",
    "    \n",
    "    # Sample features for correlation analysis\n",
    "    sample_features = feature_cols[:20] if len(feature_cols) > 20 else feature_cols\n",
    "    \n",
    "    corr_matrix = df[sample_features].corr()\n",
    "    \n",
    "    # Find highly correlated features (potential redundancy)\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > 0.8:\n",
    "                high_corr_pairs.append((\n",
    "                    corr_matrix.columns[i], \n",
    "                    corr_matrix.columns[j], \n",
    "                    corr_matrix.iloc[i, j]\n",
    "                ))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(f\"‚ö†Ô∏è High correlation pairs (>0.8): {len(high_corr_pairs)}\")\n",
    "        for pair in high_corr_pairs[:5]:  # Show first 5\n",
    "            print(f\"  {pair[0]} <-> {pair[1]}: {pair[2]:.3f}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No highly correlated features found\")\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, cmap='coolwarm', center=0, square=True, annot=False)\n",
    "    plt.title(f'Feature Correlation Matrix (First {len(sample_features)} features)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment\n",
    "if df is not None:\n",
    "    print(\"üîç DATA QUALITY ASSESSMENT FOR TDA\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check for infinite values\n",
    "    inf_counts = df.select_dtypes(include=[np.number]).apply(lambda x: np.isinf(x).sum())\n",
    "    if inf_counts.sum() > 0:\n",
    "        print(f\"‚ö†Ô∏è Infinite values found: {inf_counts[inf_counts > 0].sum()}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No infinite values\")\n",
    "    \n",
    "    # Check for zero variance features\n",
    "    zero_var_features = []\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        if df[col].std() == 0:\n",
    "            zero_var_features.append(col)\n",
    "    \n",
    "    if zero_var_features:\n",
    "        print(f\"‚ö†Ô∏è Zero variance features: {len(zero_var_features)}\")\n",
    "        print(f\"  {zero_var_features}\")\n",
    "    else:\n",
    "        print(\"‚úÖ All features have non-zero variance\")\n",
    "    \n",
    "    # Feature scale analysis\n",
    "    if 'feature_cols' in locals():\n",
    "        feature_ranges = df[feature_cols].max() - df[feature_cols].min()\n",
    "        scale_analysis = {\n",
    "            'Small scale (< 1)': sum(feature_ranges < 1),\n",
    "            'Medium scale (1-100)': sum((feature_ranges >= 1) & (feature_ranges <= 100)),\n",
    "            'Large scale (> 100)': sum(feature_ranges > 100)\n",
    "        }\n",
    "        \n",
    "        print(\"\\nüìè Feature Scale Distribution:\")\n",
    "        for scale, count in scale_analysis.items():\n",
    "            print(f\"  {scale}: {count} features\")\n",
    "        \n",
    "        if scale_analysis['Large scale (> 100)'] > 0:\n",
    "            print(\"‚ö†Ô∏è Consider feature scaling for TDA methods\")\n",
    "        else:\n",
    "            print(\"‚úÖ Feature scales relatively uniform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and next steps\n",
    "if df is not None:\n",
    "    print(\"üìã UNSW-NB15 DATASET SUMMARY FOR TDA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"‚úÖ STRENGTHS:\")\n",
    "    print(f\"  ‚Ä¢ Large scale: {len(df):,} samples for robust topology\")\n",
    "    print(f\"  ‚Ä¢ Rich features: {len(feature_cols) if 'feature_cols' in locals() else 'N/A'} numeric features\")\n",
    "    print(f\"  ‚Ä¢ Attack diversity: Multiple attack categories for signature analysis\")\n",
    "    print(f\"  ‚Ä¢ Established benchmark: Literature comparisons available\")\n",
    "    print(f\"  ‚Ä¢ Clean data: No major quality issues identified\")\n",
    "    \n",
    "    if time_cols:\n",
    "        print(f\"  ‚Ä¢ Temporal features: {len(time_cols)} time-related columns\")\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è CONSIDERATIONS:\")\n",
    "    if not time_cols:\n",
    "        print(f\"  ‚Ä¢ Limited temporal analysis: No clear timestamp columns\")\n",
    "    if 'high_corr_pairs' in locals() and high_corr_pairs:\n",
    "        print(f\"  ‚Ä¢ Feature redundancy: {len(high_corr_pairs)} highly correlated pairs\")\n",
    "    print(f\"  ‚Ä¢ Computational scale: Large dataset may require sampling for some TDA methods\")\n",
    "    \n",
    "    print(\"\\nüéØ RECOMMENDED NEXT STEPS:\")\n",
    "    print(\"1. Implement topological dissimilarity method on UNSW-NB15\")\n",
    "    print(\"2. Compare results with CTDAPD baseline (5.0% attack recall)\")\n",
    "    print(\"3. Explore persistent homology on network flow features\")\n",
    "    print(\"4. Test enhanced methods (H1/H2 homology, multiple baselines)\")\n",
    "    print(\"5. Create validation scripts following unified instructions\")\n",
    "    \n",
    "    print(\"\\nüìÅ Next notebook: 'validate_topological_dissimilarity_unsw.ipynb'\")\nelse:\n    print(\"‚ùå Cannot complete analysis - dataset not loaded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}