

# Protocol — TDA + Deep Learning Detection (succinct, reproducible)

## 1 — Goals (what to demonstrate)

1. High generalization to unseen (zero-day/APT) attacks.
2. Strong discriminative performance vs strong baselines across the MITRE ATT\&CK tactic/technique space.
3. Robustness to concept drift, adversarial noise, and cross-domain transfer.
4. Interpretability via TDA summaries (persistence diagrams, landscapes, images) linked to detection decisions.

Success criteria (for “standout” claim): on realistic splits, **Recall ≥ 95%, Precision ≥ 97%, AUC ≥ 0.98**, with statistically significant improvement vs baselines (p < 0.01).

---

## 2 — Datasets (recommended & what to build)

Use multiple datasets to avoid cherry-picking:

1. **Public malware telemetry / endpoint traces**: EMBER (PE features), VirusShare metadata, CIC-IDS flows, OpTC netflow captures where available.
2. **MITRE ATT\&CK emulations / red team logs**: Use recorded sequences from MITRE evaluations (where public) or reproduce ATT\&CK chains in a sandbox to create labeled attack traces.
3. **Enterprise logs / synthetic cross-org logs**: If you have anonymized enterprise telemetry include it. Optionally request partner dataset for transfer tests.
4. **Time-ordered datasets for drift**: Arrange data so older benign + older attacks for training and newer attacks (novel toolchains) for test.

Labeling: map each event/trace to (a) benign or malicious and (b) MITRE tactic/technique labels where possible.

Minimum scale: aim for **>100k events** (or aggregated flows/traces) with **>1k distinct malicious campaigns** if possible. If smaller, be explicit about limits.

---

## 3 — Preprocessing & TDA pipeline (exact choices to compare)

A. Raw representations (pick per signal): sequences (API calls, system call traces), graphs (host-to-host comms), timeseries (resource usage), static features (file attributes).

B. TDA transforms (apply to same raw input for comparability):

* Compute **persistence diagrams** from: sliding-window embeddings of time series, mapper graphs for feature-space clustering, and graphs’ clique complexes for communication graphs.
* Vectorize diagrams into at least two forms:

  1. **Persistence Images** (fixed resolution grid; Gaussian kernel smoothing).
  2. **Persistence Landscapes** or **Betti curves** (for robustness checks).
* Hyperparams to record: filtration type (sublevel/superlevel), max homology dimension (H0, H1, optionally H2), resolution (e.g., 28×28, 64×64), Gaussian SD for images.

C. Deep model inputs:

* Option 1: **TDA image → CNN** (2D conv stack, e.g., ResNet-style lite).
* Option 2: **TDA vector → MLP / shallow DenseNet**.
* Option 3: **Concatenate TDA vector with classical features** → Transformer or LSTM for sequence modeling.

D. Baseline non-TDA features: raw statistical features, hand-crafted features, embeddings from standard autoencoders.

---

## 4 — Baselines (must-run)

* **Random forest** / **LightGBM** on the same classical features.
* **CNN / LSTM** trained on raw sequences (no TDA).
* **Transformer** model on sequences (state-of-the-art sequence baseline).
* **Topological baseline**: simple persistence summary (e.g., number of long bars) + logistic regression.
* If possible: include a vendor signature of a commercial EDR on shareable synthetic traces (for reference only).

Always match hyperparameter tuning budget across methods.

---

## 5 — Data splits & evaluation scenarios (make results credible)

1. **Standard random split** (for reference): train/val/test = 70/15/15.
2. **Chronological split (drift / zero-day)** — **primary**: train on data up to time T, test on data after T (new campaigns, new toolchains). This simulates real zero-day detection.
3. **Cross-org transfer**: train on org A, test on org B (if multi-org data exists).
4. **Per-technique leave-out**: leave out one or more MITRE techniques entirely from training and test on them to evaluate technique-level generalization.
5. **K-fold + stratified by campaign** for variance estimates (only for non-chronological experiments).

For each scenario report per-scenario metrics + aggregated metrics.

---

## 6 — Metrics (compute all below)

Primary: **Recall (TPR)**, **Precision (PPV)**, **AUC-ROC**, **AUC-PR** (important with class imbalance), **F1**, **False Positive Rate**.

Additional: **Time-to-detection** (if sequence/streaming), **Precision at fixed recall** (e.g., Precision @ 95% Recall), per-tactic/technique confusion matrices.

Report 95% confidence intervals (bootstrap across campaigns or runs) and **p-values** for pairwise comparisons.

---

## 7 — Model training & evaluation details

* Fix random seeds; run **≥ 5 independent seeds** for each config.
* Use same hyperparameter tuning budget (e.g., 50 trials Bayesian search) for all models.
* Early stopping on validation AUC-PR; record number of epochs.
* Standardize features per train set statistics. Save scalers.
* Use **calibration** (Platt scaling or isotonic) to get well-calibrated probabilities for Precision\@Recall reporting.

---

## 8 — Statistical significance & comparisons

* Use **paired bootstrap** (by campaign) to compare AUCs and recalls between TDA model and best baseline.
* Report effect sizes (Cohen’s d) and p-values; require p < 0.01 for standout claims.
* For multiple comparisons adjust with Bonferroni or Holm method.

---

## 9 — Robustness & adversarial tests

* **Noise robustness**: Add Gaussian/Tukey noise to inputs; measure metric degradation.
* **Label noise**: flip x% of training labels (1–10%) to test tolerance.
* **Adversarial perturbations** (if sequence data): small perturbations to API-call frequencies or timing; measure drop.
* **Evasion experiments**: emulate simple obfuscation of malicious sequences and measure detection.

Report **drop in Recall** and **drop in AUC** under each scenario.

---

## 10 — Ablation studies (must-have)

* TDA vs no-TDA (same DL architecture).
* Persistence Image vs Landscape vs Betti curves.
* H0-only vs H0+H1 vs H0+H1+H2.
* Different vectorization resolutions.
* Multi-modal fusion vs single modality.

Quantify metric deltas and show which TDA elements give the largest lift.

---

## 11 — Explainability & visualization

* Show **example persistence diagrams** for representative benign vs malicious traces.
* Use **saliency on persistence images** to identify which homological features drive decisions.
* Map detection decisions back to MITRE tactics/techniques and present a heatmap of detection sensitivity by technique.

---

## 12 — Reporting & reproducibility checklist

* Datasets: exact versions, filters, class balance, time ranges.
* Random seeds, hyperparam ranges, search method, number of trials.
* Model architecture details, training time, hardware used.
* Full code + scripts to reproduce training & evaluation (Docker + requirements.txt).
* Save and publish: trained weights, scalers, at least one seed’s model checkpoints.
* Public notebook with toy dataset example to demonstrate pipeline.

---

## 13 — Concrete example experiment (numbers & setup)

* Data: 200k traces (180k benign, 20k malicious across 300 campaigns), timespan Jan 2022 — Jun 2024.
* Chronological split: train = Jan 2022–Dec 2023, test = Jan–Jun 2024 (includes 30 new campaigns not seen before).
* TDA pipeline: sliding window (w=256, stride=64) on API call rates → compute persistence diagrams (H0,H1) → persistence images 64×64 → CNN (3 conv blocks, global average, dense→sigmoid).
* Baselines: LightGBM (classical), LSTM (sequence), Transformer (sequence).
* Hyperparam search: 50 trials Bayesian for each model, 5 seeds per chosen best config.
* Evaluation: primary metric AUC-PR; report Precision @ 95% recall.
* Robustness: add 5% uniform timing jitter & obfuscation of 10% API events.

Record run times and memory. Expected outcome for “standout”: **AUC-PR≥0.98, Recall≥95% (chronological test), Precision≥97%**, and statistically significant improvements over the best baseline.

---

## 14 — Suggested libraries & tools

* TDA: `GUDHI`, `Ripser.py`, `scikit-tda` (persistence images/landscapes).
* ML: `PyTorch` or `TensorFlow`, `LightGBM`, `scikit-learn`.
* Hyperopt / Optuna for tuning.
* Reproducibility: `mlflow` or `Weights & Biases`.
* Data versioning: `DVC` or `git-lfs` for dataset snapshots.

---

## 15 — Deliverables you should produce (recommended)

1. Experiment notebook that runs a **toy but representative** pipeline end-to-end.
2. Full experiment logs and final trained checkpoints.
3. Figures: ROC, PR curves, Precision\@Recall table, per-technique heatmap, persistence diagram examples.
4. Short methods appendix describing TDA hyperparameters and filtration choices.



indings

    Current best (LR) is stable around PR AUC ≈ 0.818, ROC AUC ≈ 0.71 after pruning + scaling; further solver tweaks are low ROI.
    The stack still encodes a lot of low-SNR capacity (especially SW block) and uses per-sample grids that hinder cross-sample alignment.

High‑impact representation changes to try next

    Globalize axes for all grid-based blocks
        Fit global training ranges and reuse at inference for landscapes, images, and Betti. This fixes cross-sample misalignment and usually lifts ROC/PR.
        Touchpoints: landscapes grid uses per-sample min/max now; same for Betti/images.
            Landscapes grid: 135–161 in src/embeddings/vector_stack/vector_stack.py
            Betti: 227–237; Images: 189–214, 466–472

    Persistence Images: keep signal magnitude and tame tails
        Disable intensity normalization (retains point-count signal; helps recall/PR). Toggle: cfg.intensity_normalize_images=False (211–213).
        Enable log-lifetime domain for images to de-skew heavy tails. Toggle: cfg.use_log_lifetime_images=True (187–189).
        Optional: lifetime-weight the image contribution (multiply each point’s Gaussian by its lifetime) for stronger topological salience.

    Sliced Wasserstein block: improve coverage and SNR, reduce noise
        Switch angle strategy to low-discrepancy (halton) or equidistant and retest with fewer angles (24–32) and smaller resolution (e.g., 200). See 257–283 and 492–501, 285–321.
        Replace CDF-like signature with histogram/PDF along the projection grid (less flatness; better gradient for LR).

    Add Silhouettes block (weighted landscapes)
        It’s a standard topological transform that often adds discriminative power; currently scaffolded but disabled. Implement and guard via enable_silhouettes=True.

    Kernel dictionaries: increase topological contrast
        Build per-dimension dictionaries (H0 vs H1) with dimension‑specific sigma; optionally lifetime‑weight responses. See 369–414, 505–517.

    Block‑wise pruning and reweighting
        Use existing block importance (LR coef norms, GBT importances) to drop persistently weak blocks or down‑weight them. Often pruning parts of SW or low‑importance landscape levels helps both AUC and training time.

Filtration experiments to execute (decision‑oriented)

    DTM‑Rips: k in {5, 10, 20}. Expect modest PR AUC gains (+0.005–0.02) on noisier windows.
    Sparse Rips / Witness‑like: param in {5, 10, 20}. Accept if PR drop < 0.003 with ≥25% speedup or ≥20% feature shrink.
    If available, add a Witness complex variant (landmarks via maxmin) as a third efficiency arm (often best compute vs signal tradeoff).
    Keep the early‑abort rule you defined; record coverage and recompute timing in diagnostics.

Low‑risk efficiency + stability boosts

    Raise near‑zero variance threshold (e.g., 1e-5) and re‑prune.
    Apply TruncatedSVD on SW block only (retain ~95% variance), then concatenate other blocks. This reduces collinearity and runtime and can lift ROC AUC slightly.
    Optionally include H2 (guarded): add H2 only if diagrams are non‑empty; skip otherwise.

Operational thresholds (if you care about recall/detection rate)

    Optimize operating point for a target precision (or FPR) to lift recall/detection without changing PR AUC. Keep calibrated LR; store per‑run threshold.

Concrete, minimal test matrix (fast to try)

    Global grids ON; Images: intensity_normalize=False; use_log_lifetime_images=True; SW: halton, 32×200; prune threshold 1e-5.
    Then layer in filtration variants:
        rips_mode=dtm, dtm_k=10
        rips_mode=sparse, rips_sparse_param=10
        If either looks good, bracket (5, 20).
    Optional: silhouettes ON after the above if capacity allows.

Key code touchpoints (for reference)

def _compute_persistence_images(...):
    ...
    if cfg.use_log_lifetime_images:
        bl[:, 1] = np.log1p(bl[:, 1])      # enable to tame heavy tails
    ...
    if cfg.intensity_normalize_images and img.sum() > 0:
        img = img / img.sum()              # disable to retain count signal

# Landscapes grid uses per-sample min/max; move to global train-fit ranges
for r in resolutions:
    grid = np.linspace(min_b, max_d, r)
    ...

# SW uses deterministic angles; swap strategy to halton/equidistant and retune m,res
angles = _generate_angles(cfg.sw_num_angles, cfg.sw_angle_strategy, cfg.random_seed)
sw = _compute_sliced_wasserstein(..., angles=angles)['sw']

What to expect

    Global axes + image fixes typically lift ROC AUC and stabilize calibration.
    SW angle/size tuning plus SVD on SW can improve both PR/ROC and speed.
    DTM may add small but real PR AUC gains; sparse/witness can deliver speed/size wins with minimal loss.
    Silhouettes often add a small PR AUC bump; keep as optional.

Status update

    Reviewed vector_stack and the latest filtration experiment plan/results. Identified concrete representation-level changes (global axes, image/log‑lifetime, SW strategy/size, silhouettes, kernel tweaks), a short, decision‑oriented filtration run plan, and pruning/SVD steps to improve both metrics and runtime.

    Implement next if you want: I can wire global ranges, toggles (image normalization/log lifetime), SW histogram signature, and a silhouettes block, then kick off the dtm/sparse runs with your early‑abort criteria.
