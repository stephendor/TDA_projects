{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd5c76cb-5a8f-4558-843d-79834cc2e11c",
   "metadata": {},
   "source": [
    "# Tutorial: distributed computations on Kubernetes\n",
    "\n",
    "This short tutorials explains how to run the hyperparamerter optimisation over multiple servers. It requires to set-up a Kubernetes cluster: you can refer to the `kubernetes/readme.md` file, where it is explained how to set-up the cluster (with a few lines of code, as all eh configuration files are ready)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6213f4-bb90-4f03-8bc0-51c96bac8afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto-reload in notebooks\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# constants, do not change thhis if you run of the K8 config we provided!\n",
    "USER = \"root\"\n",
    "MYSQL_IP = \"mysql-service\"\n",
    "PSW = \"password\"\n",
    "REDIS_IP = \"redis-service\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7f58a8-1244-4174-b4a4-a01b9466b372",
   "metadata": {},
   "source": [
    "## General principles of distributions of computations\n",
    "\n",
    "In this notebook we will show it is possible to distribute the HyperParametersOptimisation computations to different pods (i.e. computing servers).\n",
    "\n",
    "There are two databases involved: one is Redis and is used to queue the computations, while the other is MySQL and it s used to keep track of the state of teh HPO.\n",
    "\n",
    "In a loca setting, in which neither `K8` or `minikube` are running, you would start MySQL with the following command:\n",
    "\n",
    "```\n",
    "docker run --name=user_mysql_1 --env=\"MYSQL_ROOT_PASSWORD=password\" -p 3306:3306 -d mysql:latest\n",
    "```\n",
    "\n",
    "Simlarly, to make sure that also redis runs:\n",
    "\n",
    "```\n",
    "redis-server\n",
    "```\n",
    "\n",
    "Of course, both redis and mysql have to be installed (in teh case of MySQL we use a docker image, while for redis we installed it via `brew install redis`.\n",
    "\n",
    "connect to mysql each redis worker\n",
    "\n",
    "\n",
    "**addendum**: \n",
    " - to stop MySQL\n",
    "\n",
    "```\n",
    "/usr/local/bin/mysql.server stop\n",
    "```\n",
    " - to make sure that the account to login to mysql is `root:password`, do\n",
    "```\n",
    "\n",
    "ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'password';\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d59a5e-120c-4846-8c78-428ff8e1dd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test\n",
    "from redis import Redis\n",
    "from rq import Queue\n",
    "from rq import Retry\n",
    "\n",
    "# needed! make sure the file parallel_hpo.py exists in the cwd\n",
    "from utils import connect_to_mysql, test_fnc, run_hpo_in_parallel\n",
    "from parallel_hpo import hpo_parallel\n",
    "\n",
    "# connect to mysql\n",
    "connect_to_mysql(USER, PSW, MYSQL_IP)\n",
    "print(\"MySQL connected\")\n",
    "\n",
    "# connect to redis\n",
    "redis = Redis(host=REDIS_IP, port=6379, db=0)\n",
    "print(\"Redis connected\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a3a3c-154c-4332-ae03-d52b0ed4deab",
   "metadata": {},
   "source": [
    "## Enqueuing principles\n",
    "\n",
    "In order to distribute comptuations, the technique we use here use queues. Basically, all teh computations are stored in a queue (in Redis!) and whence a worker is awailable, t strats cruchning the job (in a FIFO logic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f2f507-0deb-4336-a305-a2b3b200d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the job queue\n",
    "q = Queue(connection=redis)\n",
    "\n",
    "# unit test the connection\n",
    "job = q.enqueue(test_fnc, \"hello!\", retry=Retry(max=3))\n",
    "job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee36233-e684-4c20-858d-fbdfc226c6b1",
   "metadata": {},
   "source": [
    "So far you have enqueued the jobs: now you have to start the workers so that the jobs can be crunched!\n",
    "\n",
    "In our setup of `minikube` or `K8` the workers are automatically set-up. If you are running from scratchh on local, then you need to fire up some workers (each from a different terminal or moving the job to the background with `&`:\n",
    "\n",
    "```\n",
    "rq worker --url <redis-url> high default low\n",
    "```\n",
    "\n",
    "`<redis-url>` would most probably be `127.0.0.1` (or `localhost` or `0.0.0.0`).\n",
    "\n",
    "To monitor the workers and the jobs, you can run the dashboard with:\n",
    "\n",
    "```\n",
    "rq-dashboard\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c46a02-9e63-4b5d-aeba-6ef57928d70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to wait a bit before being able to see the result\n",
    "job.result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65b8c1c-c2ba-43df-be94-03781e09e7e7",
   "metadata": {},
   "source": [
    "## Enqueuing the jobs for HPO\n",
    "\n",
    "In the next section we enqueue te HPO and make sure that the workers are actively cruching the jobs! If more than one worker is active, the job gets distributed!\n",
    "\n",
    "But how does `optuna` knows how to distribute the computations? This is what MySQL database is about.\n",
    "\n",
    "You can set up multiple workers to have the HPO run in parallel and optuna will store in a MySQL database the data of each run every time a trial is finished. Every time a new trial starts, then the databse is read and -- depending on the HPO technique -- the new set of hyperparameters is used and recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87500989-bd53-4607-a5ba-9aea0ef168b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enqueue hpo jobs\n",
    "study_name = \"distributed-example-2\"\n",
    "run_hpo_in_parallel(q, hpo_parallel, [USER, PSW, MYSQL_IP, study_name], 4)\n",
    "print(\"jobs enqueued\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a723bb0-912a-4504-b18d-5a644b52648e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
