{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: Dataset Cloud\n",
    "#### Author: Raphael Reinauer\n",
    "\n",
    "This short tutorial shows how to use the DatasetCloud class to upload and\n",
    "download topological datasets in the giotto-deep framework.\n",
    " \n",
    " \n",
    "The main steps of the tutorial are the following:\n",
    " 1. Upload the dataset to the Cloud\n",
    " 2. Download the dataset from the Cloud\n",
    " 3. Automatically create dataloaders for the dataset by specifying the\n",
    "    dataset name (the dataset will be automatically downloaded from the\n",
    "    Cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This snippet will deactivate autoreload if this file\n",
    "# is run as a script and activate it if it is run as a notebook.\n",
    "from gdeep.utility.utils import autoreload_if_notebook\n",
    "\n",
    "autoreload_if_notebook()\n",
    "\n",
    "\n",
    "# Include necessary imports\n",
    "import os\n",
    "from os import remove\n",
    "from os.path import join\n",
    "\n",
    "import torch\n",
    "\n",
    "from gdeep.data.datasets import DlBuilderFromDataCloud, DatasetCloud\n",
    "from gdeep.utility.utils import get_checksum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Using the Dataset Cloud to train topological models\n",
    " In this tutorial we will use the our custom datasets storage on\n",
    " [Google Cloud Datastore](https://cloud.google.com/datastore/) to\n",
    " to store and load our datasets.\n",
    " The dataset cloud storage contain a variety of topological datasets that\n",
    " can be easily used in GDeep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see all publically available datasets, please use the following command:\n",
    "DatasetCloud(\"\").get_existing_datasets()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Uploading and downloading datasets from the cloud\n",
    " Using datasets from the cloud is very easy. The datasets are publicly\n",
    " available and can be downloaded from the cloud without any registration.\n",
    "\n",
    " To upload a dataset to the cloud, you have to have a Google Cloud API key\n",
    " to the Google Cloud Datastore bucket. If you are interested uploading your\n",
    " own dataset, please contact us at\n",
    " [raphael.reinauer@epfl.ch](mailto:raphael.reinauer@epfl.ch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_upload_dataset():\n",
    "    \"\"\"The method above creates a dataset with random data and labels,\n",
    "    saves it locally as pickled files, and then uploads it to the Cloud.\n",
    "    The dataset is then deleted from the local machine.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function does not return anything.\n",
    "    \"\"\"\n",
    "    # Generate a dataset\n",
    "    # You do not have to do that if you already have a pickled dataset\n",
    "    size_dataset = 100\n",
    "    input_dim = 5\n",
    "    num_labels = 2\n",
    "    data = torch.rand(size_dataset, input_dim)\n",
    "    labels = torch.randint(0, num_labels, (size_dataset,)).long()\n",
    "\n",
    "    # pickle data and labels\n",
    "    data_filename = \"tmp_data.pt\"\n",
    "    labels_filename = \"tmp_labels.pt\"\n",
    "    torch.save(data, data_filename)\n",
    "    torch.save(labels, labels_filename)\n",
    "\n",
    "    ## Upload dataset to Cloud\n",
    "    dataset_name = \"SmallDataset2\"\n",
    "    dataset_cloud = DatasetCloud(dataset_name, use_public_access=False)\n",
    "\n",
    "    # Specify the metadata of the dataset\n",
    "    dataset_cloud._add_metadata(\n",
    "        name=dataset_name,\n",
    "        input_size=(input_dim,),\n",
    "        size_dataset=size_dataset,\n",
    "        num_labels=num_labels,\n",
    "        data_type=\"tabular\",\n",
    "        data_format=\"pytorch_tensor\",\n",
    "    )\n",
    "\n",
    "    # upload dataset to Cloud\n",
    "    dataset_cloud._upload(data_filename, labels_filename)\n",
    "\n",
    "    # remove the labels and data files\n",
    "    # Warning: Only do this if you do want the local dataset to be deleted!\n",
    "    remove(data_filename)\n",
    "    remove(labels_filename)\n",
    "\n",
    "\n",
    "# Please only run this function if you have a Google Cloud API key\n",
    "# This is not a requirement for uploading data to the Dataset Cloud.\n",
    "\n",
    "# create_and_upload_dataset()\n",
    "\n",
    "# get base64-encoded 128-bit MD5 hash of the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Using the Dataset Cloud to train topological model\n",
    " The datasets in the cloud are automatically downloaded and used by GDeep.\n",
    " Only specify the dataset name and the path you want to save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders from data cloud\n",
    "# If you don't know what datasets exist in the cloud, just use an empty\n",
    "# ´dataset_name´ and then the error message will display all available datasets\n",
    "dataset_name = \"MutagDataset\"\n",
    "download_directory = join(\"data\", \"DatasetCloud\")\n",
    "\n",
    "dl_cloud_builder = DlBuilderFromDataCloud(dataset_name, download_directory)\n",
    "\n",
    "# You can display the metadata of the dataset using the get_metadata() method\n",
    "print(dl_cloud_builder.get_metadata())\n",
    "\n",
    "# create the dataset from the downloaded dataset\n",
    "train_dataloader, val_dataloader, test_dataloader = dl_cloud_builder.build(\n",
    "    ({\"batch_size\": 10},)\n",
    ")\n",
    "\n",
    "del train_dataloader, val_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now you can train a model on the dataset using the created dataloaders."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
