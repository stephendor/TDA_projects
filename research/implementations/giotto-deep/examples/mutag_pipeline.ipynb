{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutag dataset analysis (WIP)\n",
    "\n",
    "#### Author: Raphael Reinauer\n",
    "\n",
    "In this notebook we provide the code for the mutag analysis using Persformer.\n",
    "\n",
    "The mutag dataset ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import torch.nn as nn\n",
    "from gdeep.data import PreprocessingPipeline\n",
    "from gdeep.data.datasets import PersistenceDiagramFromFiles\n",
    "from gdeep.data.datasets.base_dataloaders import (DataLoaderBuilder,\n",
    "                                                  DataLoaderParamsTuples)\n",
    "from gdeep.data.datasets.persistence_diagrams_from_graphs_builder import \\\n",
    "    PersistenceDiagramFromGraphBuilder\n",
    "from gdeep.data.persistence_diagrams.one_hot_persistence_diagram import (\n",
    "    OneHotEncodedPersistenceDiagram, collate_fn_persistence_diagrams)\n",
    "from gdeep.data.preprocessors import (\n",
    "    FilterPersistenceDiagramByHomologyDimension,\n",
    "    FilterPersistenceDiagramByLifetime, NormalizationPersistenceDiagram)\n",
    "from gdeep.search.hpo import GiottoSummaryWriter\n",
    "from gdeep.topology_layers import Persformer, PersformerConfig, PersformerWrapper\n",
    "from gdeep.topology_layers.persformer_config import PoolerType\n",
    "from gdeep.trainer.trainer import Trainer\n",
    "from gdeep.search import HyperParameterOptimization\n",
    "from gdeep.utility import DEFAULT_GRAPH_DIR, PoolerType\n",
    "from gdeep.utility.utils import autoreload_if_notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "autoreload_if_notebook()\n",
    "\n",
    "# Parameters\n",
    "name_graph_dataset: str = 'MUTAG'\n",
    "diffusion_parameter: float = 0.1\n",
    "num_homology_types: int = 4\n",
    "\n",
    "\n",
    "# Create the persistence diagram dataset\n",
    "pd_creator = PersistenceDiagramFromGraphBuilder(name_graph_dataset, diffusion_parameter)\n",
    "pd_creator.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample extended persistence diagram\n",
    "file_path: str = os.path.join(DEFAULT_GRAPH_DIR,\n",
    "                              f\"MUTAG_{diffusion_parameter}_extended_persistence\", \"diagrams\")\n",
    "graph_idx = 1\n",
    "pd: OneHotEncodedPersistenceDiagram = \\\n",
    "    OneHotEncodedPersistenceDiagram.load(os.path.join(file_path, \n",
    "                                                      f\"{graph_idx}.npy\"))\n",
    "pd.set_homology_dimension_names([\"Ord0\", \"Ext0\", \"Rel1\", \"Ext1\"])\n",
    "pd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd_mutag_ds = PersistenceDiagramFromFiles(\n",
    "    os.path.join(\n",
    "        DEFAULT_GRAPH_DIR, f\"MUTAG_{diffusion_parameter}_extended_persistence\"\n",
    "        )\n",
    ")\n",
    "\n",
    "pd_sample: OneHotEncodedPersistenceDiagram = pd_mutag_ds[0][0]\n",
    "\n",
    "fig = pd_sample.plot([\"Ord0\", \"Ext0\", \"Rel1\", \"Ext1\"])\n",
    "# add title\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the train/validation/test split\n",
    "\n",
    "train_indices, test_indices = train_test_split(\n",
    "    range(len(pd_mutag_ds)),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "train_indices , validation_indices = train_test_split(\n",
    "    train_indices,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Create the data loaders\n",
    "train_dataset = Subset(pd_mutag_ds, train_indices)\n",
    "validation_dataset = Subset(pd_mutag_ds, validation_indices)\n",
    "test_dataset = Subset(pd_mutag_ds, test_indices)\n",
    "\n",
    "# Preprocess the data\n",
    "preprocessing_pipeline = PreprocessingPipeline[Tuple[OneHotEncodedPersistenceDiagram, int]](\n",
    "    (\n",
    "        FilterPersistenceDiagramByHomologyDimension[int]([0, 1]),\n",
    "        FilterPersistenceDiagramByLifetime[int](min_lifetime=-0.1, max_lifetime=1.0),\n",
    "        NormalizationPersistenceDiagram[int](num_homology_dimensions=4),\n",
    "     )\n",
    ")\n",
    "\n",
    "preprocessing_pipeline.fit_to_dataset(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = preprocessing_pipeline.attach_transform_to_dataset(train_dataset)  # type: ignore\n",
    "validation_dataset = preprocessing_pipeline.attach_transform_to_dataset(validation_dataset)  # type: ignore\n",
    "test_dataset = preprocessing_pipeline.attach_transform_to_dataset(test_dataset)  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dl_params = DataLoaderParamsTuples.default(\n",
    "    batch_size=32,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn_persistence_diagrams,\n",
    "    with_validation=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Build the data loaders\n",
    "dlb = DataLoaderBuilder((train_dataset, validation_dataset, test_dataset))  # type: ignore\n",
    "dl_train, dl_val, dl_test = dlb.build(dl_params)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model by using a Wrapper for the Persformer model\n",
    "wrapped_model = PersformerWrapper(\n",
    "    num_attention_layers=3,\n",
    "    num_attention_heads=4,\n",
    "    input_size= 2 + num_homology_types,\n",
    "    ouptut_size=2,\n",
    "    pooler_type=PoolerType.ATTENTION,\n",
    ")\n",
    "writer = GiottoSummaryWriter()\n",
    "\n",
    "loss_function =  nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = Trainer(wrapped_model, [dl_train, dl_val, dl_test], loss_function, writer)\n",
    "\n",
    "# initialise hpo object\n",
    "search = HyperParameterOptimization(trainer, \"accuracy\", 2, best_not_last=True)\n",
    "\n",
    "# if you want to store pickle files of the models instead of the state_dicts\n",
    "search.store_pickle = True\n",
    "\n",
    "# dictionaries of hyperparameters\n",
    "optimizers_params = {\"lr\": [0.001, 0.01]}\n",
    "dataloaders_params = {\"batch_size\": [32, 64, 16], \n",
    "                      \"collate_fn\": [collate_fn_persistence_diagrams]}\n",
    "models_hyperparams = {\n",
    "    \"num_attention_layers\": [2, 6, 1],\n",
    "    \"num_attention_heads\": [8, 16, 8],\n",
    "}\n",
    "\n",
    "# starting the HPO\n",
    "search.start(\n",
    "    [Adam],\n",
    "    3,\n",
    "    False,\n",
    "    optimizers_params,\n",
    "    dataloaders_params,\n",
    "    models_hyperparams,\n",
    "    n_accumulated_grads=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
