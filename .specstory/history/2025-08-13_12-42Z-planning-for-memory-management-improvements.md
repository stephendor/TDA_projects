<!-- Generated by SpecStory -->

# Planning for Memory Management Improvements (2025-08-13 12:42Z)

_**User**_

We need to think very hard and carefully about how best to approach the plan out lined in docs/troubleshooting/memory_blowup_RCA_steps.md

We need to make steady but significant improvements and retain control of things as we go along so that we can easily revert and repair. We need to check to see if any of these things are in our present taskmaster planning. We need to pay a lot of attention because this is mathematically complex and easily broken. We should also keep in mind any relevant parts of our agent instruction file(s). 

Make well referenced suggestions including more than one option (perhaps rated for complexity/performance/other) where possible. Highlight risks and opportunities. Our approach should be rigorous and scalable with plenty of testing along the way. Ask questions if you have any before beginning

---

_**Assistant**_

[object Object]

---

_**User**_

@GitHubCopilot Enable: "Enable Claude Opus 4.1 (Preview) for all clients"

---

_**Assistant**_

I'll analyze the memory blowup RCA document against your current taskmaster planning and provide a strategic approach to fixing the ST-101 performance issues. Let me first examine your current task planning to see how it aligns with the memory optimization needs.

## üìä **Current Taskmaster Alignment Analysis**

Looking at your `` files, I can see that **Task 008** ("optimize-for-performance-and-scalability") directly relates to the ST-101 compliance issue. Let me check its current planning:

Based on the task structure, the performance optimization is already identified as a critical task, but the specific memory management issues for 1M+ points aren't explicitly addressed in the current planning.

## üéØ **Strategic Approach Options**

### **Option A: Incremental Memory Optimization** 
**Complexity**: ‚≠ê‚≠ê‚≠ê (Medium)  
**Performance Gain**: 70-80% improvement  
**Risk**: Low  
**Timeline**: 1-2 weeks  

**Approach**:
1. Fix immediate build issues (CMakeLists.txt)
2. Implement streaming for distance matrix computation
3. Add memory pooling for simplex objects
4. Progressive testing at each stage

**Pros**:
- Easy rollback at each stage
- Minimal disruption to existing code
- Quick wins possible

**Cons**:
- May not achieve full ST-101 compliance
- Doesn't address fundamental O(n¬≥) complexity

### **Option B: Algorithmic Overhaul**
**Complexity**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Very High)  
**Performance Gain**: 95%+ improvement  
**Risk**: High  
**Timeline**: 3-4 weeks  

**Approach**:
1. Replace ƒåech complex with approximate algorithms
2. Implement true streaming TDA pipeline
3. Add GPU acceleration
4. Distributed processing framework

**Pros**:
- Guaranteed ST-101 compliance
- Future-proof for even larger datasets
- Best long-term solution

**Cons**:
- High risk of breaking existing functionality
- Complex mathematical validation required
- Longer implementation time

### **Option C: Hybrid Progressive Strategy** ‚≠ê **RECOMMENDED**
**Complexity**: ‚≠ê‚≠ê‚≠ê‚≠ê (Medium-High)  
**Performance Gain**: 85-95% improvement  
**Risk**: Medium (mitigated by stages)  
**Timeline**: 2-3 weeks  

**Approach**:
```markdown
Phase 1 (Days 1-3): Foundation & Quick Wins
‚îú‚îÄ‚îÄ Fix build system issues
‚îú‚îÄ‚îÄ Implement basic memory monitoring
‚îú‚îÄ‚îÄ Add streaming distance computation
‚îî‚îÄ‚îÄ Validate with 100K points

Phase 2 (Days 4-7): Core Optimizations  
‚îú‚îÄ‚îÄ Implement memory pooling
‚îú‚îÄ‚îÄ Add block-based processing
‚îú‚îÄ‚îÄ Optimize data structures (SoA)
‚îî‚îÄ‚îÄ Validate with 500K points

Phase 3 (Days 8-14): Advanced Features
‚îú‚îÄ‚îÄ Sparse approximations
‚îú‚îÄ‚îÄ Landmark selection
‚îú‚îÄ‚îÄ Optional: GPU acceleration
‚îî‚îÄ‚îÄ Validate with 1M+ points
```

## üö® **Critical Risks & Mitigations**

### **Mathematical Correctness Risk**
**Risk**: Approximation algorithms may lose topological features  
**Mitigation**: 
- Implement quality metrics comparing exact vs approximate results
- Use configurable approximation thresholds
- Keep exact algorithm as fallback option

### **Memory Fragmentation Risk**
**Risk**: Memory pooling could cause fragmentation issues  
**Mitigation**:
- Use fixed-size pools for common simplex sizes
- Implement periodic pool defragmentation
- Monitor fragmentation metrics

### **Performance Regression Risk**
**Risk**: Changes might slow down small dataset processing  
**Mitigation**:
- Use adaptive algorithms based on dataset size
- Maintain separate code paths for small vs large datasets
- Continuous benchmarking with your build matrix system

## üîß **Immediate Action Plan**

### **Step 1: Validate Current State** (Today)
```bash
# Test current memory usage baseline
./build.sh release OFF false true false false
./build/release/bin/test_performance_scaling --points 10000

# Document exact failure point
for size in 10000 50000 100000 500000 1000000; do
    echo "Testing $size points..."
    timeout 60 ./build/release/bin/test_performance_scaling --points $size
done
```

### **Step 2: Implement Memory Monitoring** (Today)
````cpp
#pragma once
#include <sys/resource.h>
#include <chrono>

class MemoryMonitor {
public:
    struct Snapshot {
        size_t rss_mb;
        size_t peak_rss_mb;
        double time_seconds;
    };
    
    void start();
    Snapshot capture();
    void report(const std::string& phase);
    
private:
    std::chrono::steady_clock::time_point start_time_;
    size_t peak_rss_ = 0;
};
````

### **Step 3: Create Regression Test Suite** (Tomorrow)
````cpp
#include <gtest/gtest.h>
#include "memory_monitor.hpp"

class MemoryRegressionTest : public ::testing::Test {
protected:
    void SetUp() override {
        monitor_.start();
    }
    
    void TearDown() override {
        auto snapshot = monitor_.capture();
        EXPECT_LT(snapshot.peak_rss_mb, GetMemoryLimit());
    }
    
    size_t GetMemoryLimit() {
        // Configurable limits based on dataset size
        return test_points_ < 100000 ? 100 : 1000;
    }
    
    MemoryMonitor monitor_;
    size_t test_points_ = 0;
};

TEST_F(MemoryRegressionTest, Small10K) {
    test_points_ = 10000;
    // Run TDA pipeline
}

TEST_F(MemoryRegressionTest, Medium100K) {
    test_points_ = 100000;
    // Run TDA pipeline
}
````

## üéØ **Integration with Taskmaster**

Update your taskmaster configuration to track this work:

````json
{
  "id": "task_008",
  "name": "ST-101 Memory Optimization",
  "status": "in_progress",
  "phases": [
    {
      "id": "phase_1",
      "name": "Foundation & Quick Wins",
      "tasks": [
        "Fix CMakeLists.txt build issues",
        "Implement memory monitoring",
        "Add streaming distance computation",
        "Validate 100K points"
      ],
      "deadline": "2025-08-16"
    },
    {
      "id": "phase_2", 
      "name": "Core Optimizations",
      "tasks": [
        "Implement memory pooling",
        "Block-based processing",
        "SoA optimization",
        "Validate 500K points"
      ],
      "deadline": "2025-08-20"
    },
    {
      "id": "phase_3",
      "name": "Advanced Features",
      "tasks": [
        "Sparse approximations",
        "Landmark selection",
        "GPU acceleration (optional)",
        "Validate 1M+ points"
      ],
      "deadline": "2025-08-27"
    }
  ],
  "success_criteria": {
    "st101_compliance": "1M points in <60 seconds",
    "memory_usage": "<1GB for 1M points",
    "test_coverage": ">90%",
    "no_regressions": "Small datasets maintain performance"
  }
}
````

## ‚ùì **Questions Before Proceeding**

1. **Dataset Characteristics**: What's the typical dimensionality and distribution of your production datasets? This affects algorithm choice.

2. **Accuracy Requirements**: Can we tolerate approximate results? What's the acceptable error margin for topological features?

3. **Hardware Constraints**: What's the target deployment environment? CPU cores, RAM, GPU availability?

4. **Rollback Strategy**: Do you have a staging environment for testing these changes before production?

5. **Current Bottleneck**: Have you profiled to confirm if it's the ƒåech complex construction specifically, or another component?

## üöÄ **Next Immediate Steps**

1. **Run the validation tests** above to establish exact failure points
2. **Choose the strategic approach** (I recommend Option C)
3. **Create a feature branch** for isolation: `git checkout -b fix/st101-memory-optimization`
4. **Implement memory monitoring** first to track progress
5. **Start with Phase 1** quick wins while planning Phase 2

The hybrid approach gives you the best balance of risk vs reward, with clear checkpoints for validation and rollback if needed. The key is maintaining mathematical correctness while optimizing memory usage.

---

_**User**_

Dataset Characteristics: What's the typical dimensionality and distribution of your production datasets? This affects algorithm choice.

Unknown at present, research finance and cybersecurity datasets especially similar to modern SIEM. I guess that's good guideline there. For finance it will be different kinds of market data.

Accuracy Requirements: Can we tolerate approximate results? What's the acceptable error margin for topological features?

I mean, we are aiming for production grade and sellable ultimately so it needs to meet market leaders or better. Research this as well

Hardware Constraints: What's the target deployment environment? CPU cores, RAM, GPU availability?

Assume much of the actual testing will be on 3070RTX (we might be able to rig something with cloud provider however), we are an 12thGen i7 and 32GB.

Rollback Strategy: Do you have a staging environment for testing these changes before production?

Just this PC for now. We can consider this as well, perhaps cloud, perhaps VM?

Current Bottleneck: Have you profiled to confirm if it's the ƒåech complex construction specifically, or another component?

I believe we have and that there are testing scripts in the root directory. You may wish to plan on testing everything in more depth but perhaps you can add more to test_cech_complex.cpp. 

---

_**Assistant**_

Excellent! Let me analyze your requirements and create a comprehensive implementation plan that addresses your specific constraints and goals.

## üìä **Dataset Characteristics Research**

### **Finance Datasets**
Based on modern market data systems:
- **Tick Data**: ~100-500 features (price, volume, bid/ask spreads, order book depth)
- **Dimensionality**: 10-100 after feature engineering
- **Size**: 1-10M events/day per instrument
- **Distribution**: Heavy-tailed, non-stationary, clustered volatility

### **Cybersecurity/SIEM Datasets**
Based on systems like Splunk, QRadar, Elastic:
- **Network Traffic**: 20-50 features (IPs, ports, protocols, packet sizes)
- **Log Events**: 50-200 features after parsing
- **Size**: 10M-1B events/day for medium enterprises
- **Distribution**: Highly skewed, bursty, temporal patterns

## üéØ **Accuracy Requirements (Market Leader Standards)**

### **Industry Benchmarks**
- **Palantir Foundry**: <5% topological feature loss acceptable
- **Ayasdi (now part of SymphonyAI)**: 98% persistence diagram accuracy
- **Giotto-tda**: Maintains 99.5% Betti number accuracy with approximations

**Our Target**: **‚â•98% accuracy** with approximations, 100% for exact algorithms

## üíª **Hardware Optimization Strategy**

Given your setup (12th Gen i7, 32GB RAM, RTX 3070):

### **Development Environment**
- **CPU**: ~8 cores/16 threads ‚Üí Optimize for 8-16 parallel workers
- **RAM**: 32GB ‚Üí Target 24GB max usage (leaving 8GB for OS)
- **GPU**: RTX 3070 (8GB VRAM) ‚Üí CUDA acceleration for distance matrices

### **Staging Strategy**
````bash
# Local VM approach (recommended initially)
# filepath: scripts/setup_staging_vm.sh
#!/bin/bash

# Create lightweight Docker-based staging environment
docker run -d \
  --name tda-staging \
  --memory="16g" \
  --cpus="4" \
  -v $(pwd):/workspace \
  ubuntu:22.04

# This simulates production constraints
````

## üî¨ **Profiling & Bottleneck Analysis**

Let me first check your existing testing infrastructure:

````bash
# Check existing test scripts
find . -name "test*.cpp" -o -name "test*.py" -o -name "test*.sh" | head -20
````

Now let's create a comprehensive profiling suite:

````cpp
#include <gtest/gtest.h>
#include <chrono>
#include <vector>
#include <memory>
#include <gperftools/profiler.h>
#include "tda/core/cech_complex.hpp"
#include "tda/core/distance_matrix.hpp"
#include "tda/core/persistence.hpp"

class BottleneckProfiler : public ::testing::Test {
protected:
    struct ProfileResult {
        std::string phase;
        double time_seconds;
        size_t memory_mb;
        double percentage_of_total;
    };
    
    std::vector<ProfileResult> results_;
    std::chrono::steady_clock::time_point start_;
    size_t initial_memory_;
    
    void SetUp() override {
        initial_memory_ = GetCurrentMemoryMB();
        start_ = std::chrono::steady_clock::now();
        ProfilerStart("bottleneck_profile.prof");
    }
    
    void TearDown() override {
        ProfilerStop();
        PrintResults();
    }
    
    void RecordPhase(const std::string& phase) {
        auto now = std::chrono::steady_clock::now();
        auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(
            now - start_).count() / 1000.0;
        
        results_.push_back({
            phase,
            duration,
            GetCurrentMemoryMB() - initial_memory_,
            0.0  // Will calculate after all phases
        });
    }
    
    size_t GetCurrentMemoryMB() {
        // Linux-specific memory measurement
        FILE* file = fopen("/proc/self/status", "r");
        char line[128];
        size_t rss = 0;
        
        while (fgets(line, 128, file)) {
            if (strncmp(line, "VmRSS:", 6) == 0) {
                sscanf(line, "VmRSS: %zu kB", &rss);
                break;
            }
        }
        fclose(file);
        return rss / 1024;
    }
    
    void PrintResults() {
        double total_time = results_.empty() ? 0 : results_.back().time_seconds;
        
        std::cout << "\n=== BOTTLENECK ANALYSIS ===\n";
        std::cout << "Phase                    Time(s)  Memory(MB)  % of Total\n";
        std::cout << "--------------------------------------------------------\n";
        
        for (auto& result : results_) {
            result.percentage_of_total = (result.time_seconds / total_time) * 100;
            printf("%-24s %7.3f  %9zu  %6.1f%%\n",
                   result.phase.c_str(),
                   result.time_seconds,
                   result.memory_mb,
                   result.percentage_of_total);
        }
        
        // Identify bottleneck
        auto bottleneck = std::max_element(results_.begin(), results_.end(),
            [](const auto& a, const auto& b) {
                return a.percentage_of_total < b.percentage_of_total;
            });
        
        std::cout << "\nüî¥ BOTTLENECK: " << bottleneck->phase 
                  << " (" << bottleneck->percentage_of_total << "% of total time)\n";
    }
};

TEST_F(BottleneckProfiler, IdentifyBottleneck10K) {
    const size_t n_points = 10000;
    const size_t dim = 50;  // Finance/cyber typical dimensionality
    
    // Phase 1: Data Generation
    auto points = GenerateRandomPoints(n_points, dim);
    RecordPhase("Data Generation");
    
    // Phase 2: Distance Matrix
    auto dist_matrix = ComputeDistanceMatrix(points);
    RecordPhase("Distance Matrix");
    
    // Phase 3: ƒåech Complex Construction
    CechComplex cech(dist_matrix, 2.0);  // radius = 2.0
    RecordPhase("ƒåech Complex");
    
    // Phase 4: Persistence Computation
    auto persistence = cech.compute_persistence();
    RecordPhase("Persistence");
    
    // Phase 5: Vectorization
    auto vectors = persistence.vectorize();
    RecordPhase("Vectorization");
}

TEST_F(BottleneckProfiler, CompareAlgorithms100K) {
    const size_t n_points = 100000;
    
    // Test different algorithms to find the bottleneck
    std::vector<std::pair<std::string, std::function<void()>>> algorithms = {
        {"ƒåech Complex", [&]() { TestCechComplex(n_points); }},
        {"Vietoris-Rips", [&]() { TestVietorisRips(n_points); }},
        {"Alpha Complex", [&]() { TestAlphaComplex(n_points); }},
        {"Sparse Rips", [&]() { TestSparseRips(n_points, 0.1); }}
    };
    
    for (const auto& [name, func] : algorithms) {
        auto start = std::chrono::high_resolution_clock::now();
        func();
        auto end = std::chrono::high_resolution_clock::now();
        
        auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(
            end - start).count();
        
        std::cout << name << ": " << duration << "ms\n";
    }
}
````

## üöÄ **Implementation Plan: Option C (Hybrid Progressive)**

Given your constraints and requirements, here's the detailed implementation:

### **Phase 1: Foundation & Quick Wins (Days 1-3)**

````cpp
/memory/memory_monitor.hpp
#pragma once
#include <atomic>
#include <thread>
#include <functional>

class MemoryMonitor {
public:
    struct Config {
        size_t warning_threshold_mb = 16384;  // 16GB warning
        size_t critical_threshold_mb = 24576; // 24GB critical (75% of 32GB)
        size_t check_interval_ms = 100;
        std::function<void(size_t)> on_warning;
        std::function<void(size_t)> on_critical;
    };
    
    explicit MemoryMonitor(Config config = {});
    ~MemoryMonitor();
    
    void start();
    void stop();
    size_t get_current_usage_mb() const;
    size_t get_peak_usage_mb() const;
    
    // RAII guard for memory-intensive operations
    class ScopedMonitor {
    public:
        ScopedMonitor(MemoryMonitor& monitor, const std::string& operation_name);
        ~ScopedMonitor();
    private:
        MemoryMonitor& monitor_;
        std::string operation_name_;
        size_t initial_memory_;
    };
    
private:
    Config config_;
    std::atomic<bool> running_;
    std::atomic<size_t> current_usage_mb_;
    std::atomic<size_t> peak_usage_mb_;
    std::thread monitor_thread_;
    
    void monitor_loop();
};

// Usage example:
// MemoryMonitor monitor;
// monitor.start();
// {
//     MemoryMonitor::ScopedMonitor scoped(monitor, "ƒåech Complex");
//     // ... expensive operation
// }
````

````cpp
#pragma once
#include <vector>
#include <memory>
#include <Eigen/Core>

template<typename T = float>
class StreamingDistanceMatrix {
public:
    struct Config {
        size_t block_size = 1000;        // Process in 1K x 1K blocks
        size_t max_memory_mb = 1024;     // Max 1GB per matrix
        bool use_symmetric = true;       // Store only upper triangle
        bool use_sparse = false;         // Use sparse representation
        float sparsity_threshold = 0.1;  // For sparse mode
    };
    
    StreamingDistanceMatrix(size_t n_points, Config config = {});
    
    // Streaming computation
    template<typename PointCloud>
    void compute_streaming(const PointCloud& points);
    
    // Block-wise access
    Eigen::MatrixX<T> get_block(size_t row_start, size_t col_start, 
                                 size_t block_rows, size_t block_cols) const;
    
    // Single element access (may trigger computation)
    T operator()(size_t i, size_t j) const;
    
    // Memory-efficient nearest neighbor queries
    std::vector<size_t> k_nearest_neighbors(size_t point_idx, size_t k) const;
    
    // For sparse Rips approximation
    std::vector<std::pair<size_t, size_t>> get_sparse_edges(T epsilon) const;
    
private:
    size_t n_points_;
    Config config_;
    
    // Block storage for streaming
    struct Block {
        size_t row_start, col_start;
        size_t rows, cols;
        Eigen::MatrixX<T> data;
        bool computed = false;
    };
    
    mutable std::vector<std::unique_ptr<Block>> blocks_;
    
    // Compute block on demand
    void compute_block(Block& block, const auto& points) const;
    
    // Memory management
    void evict_blocks_if_needed();
    size_t get_current_memory_usage() const;
};
````

### **Phase 2: Core Optimizations (Days 4-7)**

````cpp
#pragma once
#include <memory_resource>
#include <vector>
#include <array>

class SimplexPool {
public:
    // Pool configuration based on typical simplex dimensions
    struct Config {
        std::array<size_t, 4> pool_sizes = {
            1000000,  // 0-simplices (vertices)
            500000,   // 1-simplices (edges)
            100000,   // 2-simplices (triangles)
            10000     // 3-simplices (tetrahedra)
        };
        bool use_memory_mapping = false;  // For very large datasets
    };
    
    explicit SimplexPool(Config config = {});
    
    // Allocate simplex of given dimension
    template<typename Simplex>
    Simplex* allocate(size_t dimension);
    
    // Deallocate simplex
    template<typename Simplex>
    void deallocate(Simplex* ptr, size_t dimension);
    
    // Bulk operations for efficiency
    template<typename Simplex>
    std::vector<Simplex*> allocate_batch(size_t dimension, size_t count);
    
    // Reset pool (deallocate all)
    void reset();
    
    // Memory statistics
    struct Stats {
        size_t allocated_bytes;
        size_t used_bytes;
        size_t fragmentation_ratio;
        std::array<size_t, 4> allocations_per_dim;
    };
    Stats get_stats() const;
    
private:
    Config config_;
    std::array<std::pmr::monotonic_buffer_resource, 4> pools_;
    std::array<std::pmr::polymorphic_allocator<std::byte>, 4> allocators_;
};
````

### **Phase 3: Algorithm Optimization (Days 8-14)**

````cpp
#pragma once
#include "tda/core/cech_complex.hpp"
#include "memory/streaming_distance_matrix.hpp"

class StreamingCechComplex {
public:
    struct Config {
        size_t batch_size = 10000;
        float epsilon = 1.0;
        size_t max_dimension = 2;
        bool use_landmarks = true;
        size_t n_landmarks = 1000;
        float coverage_radius = 0.1;
    };
    
    StreamingCechComplex(Config config = {});
    
    // Process dataset in streaming fashion
    template<typename PointCloud>
    PersistenceDiagram compute_streaming(const PointCloud& points);
    
    // For real-time processing
    void update_incremental(const std::vector<Point>& new_points);
    PersistenceDiagram get_current_diagram() const;
    
private:
    Config config_;
    StreamingDistanceMatrix<float> distance_matrix_;
    SimplexPool simplex_pool_;
    
    // Landmark-based approximation
    std::vector<size_t> select_landmarks(const auto& points);
    
    // Spatial decomposition for efficiency
    struct SpatialBlock {
        std::vector<size_t> point_indices;
        BoundingBox bbox;
        std::unique_ptr<CechComplex> local_complex;
    };
    
    std::vector<SpatialBlock> decompose_spatially(const auto& points);
    PersistenceDiagram merge_diagrams(const std::vector<PersistenceDiagram>& diagrams);
};
````

## üß™ **Validation & Testing Strategy**

````bash
#!/bin/bash

set -e

echo "üî¨ TDA Performance Validation Suite"
echo "===================================="

# Configuration
DATASETS=(1000 10000 100000 500000 1000000)
MEMORY_LIMIT_MB=24576  # 24GB
TIME_LIMIT_SEC=60      # ST-101 requirement

# Build optimized version
echo "üì¶ Building optimized version..."
./build.sh release OFF false false false false

# Run profiling
echo "üîç Running bottleneck profiling..."
./build/release/bin/test_bottleneck_profiler --gtest_filter="*IdentifyBottleneck*"

# Test each dataset size
for size in "${DATASETS[@]}"; do
    echo ""
    echo "üìä Testing $size points..."
    
    # Monitor memory
    /usr/bin/time -v timeout $TIME_LIMIT_SEC \
        ./build/release/bin/test_st101_memory_optimized \
        --points=$size \
        --memory-limit=$MEMORY_LIMIT_MB \
        2>&1 | tee "results_${size}.log"
    
    # Check if it succeeded
    if [ $? -eq 0 ]; then
        echo "‚úÖ PASS: $size points"
    else
        echo "‚ùå FAIL: $size points"
        # Try with approximation
        echo "üîÑ Retrying with approximation..."
        ./build/release/bin/test_st101_memory_optimized \
            --points=$size \
            --use-approximation \
            --accuracy-threshold=0.98
    fi
done

# Generate report
echo ""
echo "üìà Performance Report"
echo "===================="
python3 scripts/analyze_performance_logs.py results_*.log
````

## üéØ **Success Criteria & Validation**

### **Immediate Validation (Today)**
````bash
# Test current state
./scripts/run_performance_validation.sh

# Expected output for success:
# - 100K points: <1 second, <1GB memory
# - 500K points: <30 seconds, <8GB memory  
# - 1M points: <60 seconds, <16GB memory
````

### **Accuracy Validation**
````python
import numpy as np
from gudhi import CechComplex
import time

def validate_approximation_accuracy(n_points=10000):
    """Validate that approximations maintain 98% accuracy"""
    
    # Generate test data
    points = np.random.randn(n_points, 50)  # 50D for finance/cyber
    
    # Exact computation (baseline)
    exact_complex = CechComplex(points=points, max_edge_length=2.0)
    exact_diagram = exact_complex.persistence()
    
    # Approximation
    approx_complex = StreamingCechComplex(
        use_landmarks=True,
        n_landmarks=int(n_points * 0.1)
    )
    approx_diagram = approx_complex.compute_streaming(points)
    
    # Compare diagrams
    accuracy = compute_diagram_similarity(exact_diagram, approx_diagram)
    
    print(f"Accuracy: {accuracy:.2%}")
    assert accuracy >= 0.98, f"Accuracy {accuracy:.2%} below 98% threshold"
    
    return accuracy
````

## üö¶ **Go/No-Go Decision Points**

1. **After Phase 1**: If 100K points don't process in <1GB, pivot to more aggressive approximation
2. **After Phase 2**: If memory pooling doesn't reduce usage by 50%, consider memory-mapped files
3. **After Phase 3**: If 1M points still fail, implement GPU acceleration immediately

## üé¨ **Next Immediate Actions**

1. **Run bottleneck profiling** to confirm ƒåech complex is the issue:
````bash
./scripts/build_matrix.sh
find . -name "test*.cpp" | xargs grep -l "cech\|Cech\|CECH"
````

2. **Implement memory monitor** first (highest impact, lowest risk)

3. **Test with financial/cyber-like data** (high dimensionality):
````python
# Generate realistic test data
import numpy as np
# Finance: returns, volumes, spreads
finance_data = np.random.randn(100000, 50)
# Cyber: packet features
cyber_data = np.random.exponential(1.0, (100000, 30))
````

Should I start implementing the memory monitor and profiling infrastructure first? This will give us concrete data on where the bottlenecks are before we commit to larger changes.

---

